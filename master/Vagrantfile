# -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|
  config.vm.box = "centos/7"
  config.vm.hostname = "master"

  # special
  config.vm.network "private_network", ip: "10.0.15.10"
  # config.vm.network "forwarded_port", guest: 9090, host: 9090, host_ip: "127.0.0.1"

  config.vm.provider "virtualbox" do |vb|
    vb.gui = false
    vb.memory = "4096"
    vb.cpus = 2
    vb.name = "master"

    # exec cap: even if the machine works at 100%+ cpu, it won't affect the host device too much, causing it's overheat
    vb.customize ["modifyvm", :id, "--cpuexecutioncap", "50"]
  end

  config.vm.provision "shell", privileged: false, inline: <<-SHELL

    UID=vagrant
    GID=vagrant
    HOME=/home/vagrant

    sudo yum install -y sshpass wget curl;

    # prepare
    printf "\n10.0.15.10 master\n10.0.15.21 node01\n10.0.15.22 node02\n" | sudo tee -a /etc/hosts;
    sudo setenforce 0;
    sudo sed -i --follow-symlinks 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux;
    sudo modprobe br_netfilter;
    echo "br_netfilter" | sudo tee -a /etc/modules-load.d/br_netfilter.conf;

    echo '1' | sudo tee -a /proc/sys/net/bridge/bridge-nf-call-iptables;
    printf "\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward=1\n" | sudo tee -a /etc/sysctl.conf;
    sudo sysctl -p;

    sudo systemctl stop firewalld;
    sudo systemctl disable firewalld;
    sudo systemctl mask --now firewalld;

    sudo swapoff -a;
    sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab;

    # install Docker
    sudo yum install -y yum-utils device-mapper-persistent-data lvm2;
    sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo;
    sudo yum install -y docker-ce;

    sudo mkdir /etc/docker;
    sudo tee /etc/docker/daemon.json > /dev/null <<'EOF'
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ]
}
EOF

    # install kubernetes
    sudo tee /etc/yum.repos.d/kubernetes.repo > /dev/null <<'EOF'
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
    sudo yum install -y kubelet kubeadm kubectl;

    # start all
    sudo systemctl start docker;
    sudo systemctl enable docker;
    sudo systemctl start kubelet;
    sudo systemctl enable kubelet;

    # initialize the cluster
    sudo kubeadm init --apiserver-advertise-address=10.0.15.10 --pod-network-cidr=10.244.0.0/16 --apiserver-cert-extra-sans=localhost,127.0.0.1;
    mkdir -p ${HOME}/.kube;
    sudo cp -i /etc/kubernetes/admin.conf ${HOME}/.kube/config;
    sudo chown ${UID}:${$GID} ${HOME}/.kube/config;

    # flannel network
    kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml;

    # nginx ingress & load balancer
    kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.8.3/manifests/metallb.yaml
    cat <<EOF | kubectl apply -f -
    tee /tmp/metallb-conf.yml > /dev/null <<'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
    namespace: metallb-system
    name: config
data:
    config: |
        address-pools:
        - name: default
          protocol: layer2
          addresses:
          - 10.0.15.21-10.0.15.22
EOF
    kubectl apply -f /tmp/metallb-conf.yml;
    rm /tmp/metallb-conf.yml;

    kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml;
    kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/cloud-generic.yaml;
    kubectl get pods --all-namespaces -l app.kubernetes.io/name=ingress-nginx;

    ###############################################################
    ### Helm

    curl -L https://git.io/get_helm.sh | bash
    kubectl -n kube-system create serviceaccount tiller
    kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
    helm init --client-only

    # (see: https://v2.helm.sh/docs/using_helm/#securing-your-helm-installation on how to run tiller locally)
    # first terminal:
    # tiller --storage=secret
    # second terminal:
    # export HELM_HOST=:44134

    # kubectl port-forward -n monitoring prometheus-prometheus-operator-prometheus-0 9090 --address 0.0.0.0
    # kubectl port-forward $(kubectl get pods --selector=app=grafana -n monitoring --output=jsonpath="{.items..metadata.name}") -n monitoring 3000 --address 0.0.0.0

    # additional stuff
    # helm reset --force
    # helm init --service-account tiller --tiller-tls-verify

    ###############################################################
    ### Prometheus
    helm install stable/prometheus-operator --name prometheus-operator --namespace monitoring
    # kubectl --namespace monitoring get pods -l "release=prometheus-operator"

    # expose Grafana
    tee /tmp/grafana-ingress.yml > /dev/null <<'EOF'
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: monitoring
  namespace: monitoring
spec:
  rules:
  - host: your-domain.com
    http:
      paths:
      - backend:
          serviceName: prometheus-operator-grafana
          servicePort: 80
EOF
    kubectl apply -f /tmp/grafana-ingress.yml;
    rm /tmp/grafana-ingress.yml;

    ###############################################################
    ### The following block is for the development cluster only

    # ssh with password
    sudo sed -i -e 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config;
    sudo usermod --password $(echo 123 | openssl passwd -1 -stdin) vagrant;
    sudo service sshd restart;

    # enable running docker cli without sudo (for development only):
    sudo usermod -aG docker ${UID};

    # generate access token
    kubeadm token create --print-join-command > /tmp/kubeadm_join_cmd.sh;
    chmod +x /tmp/kubeadm_join_cmd.sh;

    ###############################################################
    ### This might be helpful on production
    # sudo firewall-cmd --zone=public --add-port=6443/tcp --permanent
    # sudo firewall-cmd --zone=public --add-port=10250/tcp --permanent
    ### also this
    # sudo iptables -P FORWARD ACCEPT
    ### or this :) Mambo-Jambo
    # sudo  /sbin/iptables -I FORWARD 1 -i cni0 -j ACCEPT -m comment --comment "flannel subnet"
    # sudo  /sbin/iptables -I FORWARD 1 -o cni0 -j ACCEPT -m comment --comment "flannel subnet"
    # sudo /sbin/iptables -t nat -A POSTROUTING -s 10.244.0.0/16 ! -d 10.244.0.0/16 -j MASQUERADE

  SHELL
end
